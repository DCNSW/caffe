I1016 22:55:39.456691 32678 caffe.cpp:218] Using GPUs 0
I1016 22:55:39.566294 32678 caffe.cpp:223] GPU 0: GeForce GTX TITAN Black
I1016 22:55:40.586542 32678 solver.cpp:44] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "../examples/mnist/build/snapshot/lenet"
solver_mode: GPU
device_id: 0
net: "../examples/mnist/build/train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I1016 22:55:40.601542 32678 solver.cpp:87] Creating training net from net file: ../examples/mnist/build/train_test.prototxt
I1016 22:55:40.619194 32678 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I1016 22:55:40.619206 32678 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1016 22:55:40.619266 32678 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "../examples/mnist/data/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1016 22:55:40.619312 32678 layer_factory.hpp:77] Creating layer mnist
I1016 22:55:40.690330 32678 db_lmdb.cpp:35] Opened lmdb ../examples/mnist/data/mnist_train_lmdb
I1016 22:55:40.697499 32678 net.cpp:84] Creating Layer mnist
I1016 22:55:40.697518 32678 net.cpp:380] mnist -> data
I1016 22:55:40.697541 32678 net.cpp:380] mnist -> label
I1016 22:55:40.697963 32678 data_layer.cpp:45] output data size: 64,1,28,28
I1016 22:55:40.698900 32678 net.cpp:122] Setting up mnist
I1016 22:55:40.698911 32678 net.cpp:129] Top shape: 64 1 28 28 (50176)
I1016 22:55:40.698915 32678 net.cpp:129] Top shape: 64 (64)
I1016 22:55:40.698916 32678 net.cpp:137] Memory required for data: 200960
I1016 22:55:40.698921 32678 layer_factory.hpp:77] Creating layer conv1
I1016 22:55:40.698936 32678 net.cpp:84] Creating Layer conv1
I1016 22:55:40.698953 32678 net.cpp:406] conv1 <- data
I1016 22:55:40.698962 32678 net.cpp:380] conv1 -> conv1
I1016 22:55:42.181859 32678 net.cpp:122] Setting up conv1
I1016 22:55:42.181879 32678 net.cpp:129] Top shape: 64 20 24 24 (737280)
I1016 22:55:42.181882 32678 net.cpp:137] Memory required for data: 3150080
I1016 22:55:42.181902 32678 layer_factory.hpp:77] Creating layer pool1
I1016 22:55:42.181926 32678 net.cpp:84] Creating Layer pool1
I1016 22:55:42.181931 32678 net.cpp:406] pool1 <- conv1
I1016 22:55:42.181936 32678 net.cpp:380] pool1 -> pool1
I1016 22:55:42.181975 32678 net.cpp:122] Setting up pool1
I1016 22:55:42.181980 32678 net.cpp:129] Top shape: 64 20 12 12 (184320)
I1016 22:55:42.181982 32678 net.cpp:137] Memory required for data: 3887360
I1016 22:55:42.181984 32678 layer_factory.hpp:77] Creating layer conv2
I1016 22:55:42.181993 32678 net.cpp:84] Creating Layer conv2
I1016 22:55:42.181994 32678 net.cpp:406] conv2 <- pool1
I1016 22:55:42.181999 32678 net.cpp:380] conv2 -> conv2
I1016 22:55:42.183182 32678 net.cpp:122] Setting up conv2
I1016 22:55:42.183192 32678 net.cpp:129] Top shape: 64 50 8 8 (204800)
I1016 22:55:42.183193 32678 net.cpp:137] Memory required for data: 4706560
I1016 22:55:42.183199 32678 layer_factory.hpp:77] Creating layer pool2
I1016 22:55:42.183203 32678 net.cpp:84] Creating Layer pool2
I1016 22:55:42.183207 32678 net.cpp:406] pool2 <- conv2
I1016 22:55:42.183209 32678 net.cpp:380] pool2 -> pool2
I1016 22:55:42.183233 32678 net.cpp:122] Setting up pool2
I1016 22:55:42.183238 32678 net.cpp:129] Top shape: 64 50 4 4 (51200)
I1016 22:55:42.183239 32678 net.cpp:137] Memory required for data: 4911360
I1016 22:55:42.183241 32678 layer_factory.hpp:77] Creating layer ip1
I1016 22:55:42.183248 32678 net.cpp:84] Creating Layer ip1
I1016 22:55:42.183249 32678 net.cpp:406] ip1 <- pool2
I1016 22:55:42.183264 32678 net.cpp:380] ip1 -> ip1
I1016 22:55:42.185214 32678 net.cpp:122] Setting up ip1
I1016 22:55:42.185221 32678 net.cpp:129] Top shape: 64 500 (32000)
I1016 22:55:42.185223 32678 net.cpp:137] Memory required for data: 5039360
I1016 22:55:42.185228 32678 layer_factory.hpp:77] Creating layer relu1
I1016 22:55:42.185235 32678 net.cpp:84] Creating Layer relu1
I1016 22:55:42.185236 32678 net.cpp:406] relu1 <- ip1
I1016 22:55:42.185240 32678 net.cpp:367] relu1 -> ip1 (in-place)
I1016 22:55:42.185576 32678 net.cpp:122] Setting up relu1
I1016 22:55:42.185583 32678 net.cpp:129] Top shape: 64 500 (32000)
I1016 22:55:42.185585 32678 net.cpp:137] Memory required for data: 5167360
I1016 22:55:42.185588 32678 layer_factory.hpp:77] Creating layer ip2
I1016 22:55:42.185593 32678 net.cpp:84] Creating Layer ip2
I1016 22:55:42.185595 32678 net.cpp:406] ip2 <- ip1
I1016 22:55:42.185600 32678 net.cpp:380] ip2 -> ip2
I1016 22:55:42.185966 32678 net.cpp:122] Setting up ip2
I1016 22:55:42.185974 32678 net.cpp:129] Top shape: 64 10 (640)
I1016 22:55:42.185976 32678 net.cpp:137] Memory required for data: 5169920
I1016 22:55:42.185981 32678 layer_factory.hpp:77] Creating layer loss
I1016 22:55:42.185986 32678 net.cpp:84] Creating Layer loss
I1016 22:55:42.185987 32678 net.cpp:406] loss <- ip2
I1016 22:55:42.185991 32678 net.cpp:406] loss <- label
I1016 22:55:42.186005 32678 net.cpp:380] loss -> loss
I1016 22:55:42.186015 32678 layer_factory.hpp:77] Creating layer loss
I1016 22:55:42.186185 32678 net.cpp:122] Setting up loss
I1016 22:55:42.186192 32678 net.cpp:129] Top shape: (1)
I1016 22:55:42.186194 32678 net.cpp:132]     with loss weight 1
I1016 22:55:42.186205 32678 net.cpp:137] Memory required for data: 5169924
I1016 22:55:42.186208 32678 net.cpp:198] loss needs backward computation.
I1016 22:55:42.186210 32678 net.cpp:198] ip2 needs backward computation.
I1016 22:55:42.186211 32678 net.cpp:198] relu1 needs backward computation.
I1016 22:55:42.186213 32678 net.cpp:198] ip1 needs backward computation.
I1016 22:55:42.186215 32678 net.cpp:198] pool2 needs backward computation.
I1016 22:55:42.186218 32678 net.cpp:198] conv2 needs backward computation.
I1016 22:55:42.186219 32678 net.cpp:198] pool1 needs backward computation.
I1016 22:55:42.186221 32678 net.cpp:198] conv1 needs backward computation.
I1016 22:55:42.186233 32678 net.cpp:200] mnist does not need backward computation.
I1016 22:55:42.186235 32678 net.cpp:242] This network produces output loss
I1016 22:55:42.186242 32678 net.cpp:255] Network initialization done.
I1016 22:55:42.186375 32678 solver.cpp:172] Creating test net (#0) specified by net file: ../examples/mnist/build/train_test.prototxt
I1016 22:55:42.186399 32678 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I1016 22:55:42.186475 32678 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "../examples/mnist/data/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I1016 22:55:42.186524 32678 layer_factory.hpp:77] Creating layer mnist
I1016 22:55:42.234812 32678 db_lmdb.cpp:35] Opened lmdb ../examples/mnist/data/mnist_test_lmdb
I1016 22:55:42.241120 32678 net.cpp:84] Creating Layer mnist
I1016 22:55:42.241148 32678 net.cpp:380] mnist -> data
I1016 22:55:42.241161 32678 net.cpp:380] mnist -> label
I1016 22:55:42.241266 32678 data_layer.cpp:45] output data size: 100,1,28,28
I1016 22:55:42.242949 32678 net.cpp:122] Setting up mnist
I1016 22:55:42.242980 32678 net.cpp:129] Top shape: 100 1 28 28 (78400)
I1016 22:55:42.242985 32678 net.cpp:129] Top shape: 100 (100)
I1016 22:55:42.242985 32678 net.cpp:137] Memory required for data: 314000
I1016 22:55:42.242990 32678 layer_factory.hpp:77] Creating layer label_mnist_1_split
I1016 22:55:42.242995 32678 net.cpp:84] Creating Layer label_mnist_1_split
I1016 22:55:42.242998 32678 net.cpp:406] label_mnist_1_split <- label
I1016 22:55:42.243005 32678 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_0
I1016 22:55:42.243010 32678 net.cpp:380] label_mnist_1_split -> label_mnist_1_split_1
I1016 22:55:42.243127 32678 net.cpp:122] Setting up label_mnist_1_split
I1016 22:55:42.243149 32678 net.cpp:129] Top shape: 100 (100)
I1016 22:55:42.243152 32678 net.cpp:129] Top shape: 100 (100)
I1016 22:55:42.243155 32678 net.cpp:137] Memory required for data: 314800
I1016 22:55:42.243157 32678 layer_factory.hpp:77] Creating layer conv1
I1016 22:55:42.243180 32678 net.cpp:84] Creating Layer conv1
I1016 22:55:42.243186 32678 net.cpp:406] conv1 <- data
I1016 22:55:42.243191 32678 net.cpp:380] conv1 -> conv1
I1016 22:55:42.244792 32678 net.cpp:122] Setting up conv1
I1016 22:55:42.244801 32678 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I1016 22:55:42.244812 32678 net.cpp:137] Memory required for data: 4922800
I1016 22:55:42.244834 32678 layer_factory.hpp:77] Creating layer pool1
I1016 22:55:42.244849 32678 net.cpp:84] Creating Layer pool1
I1016 22:55:42.244851 32678 net.cpp:406] pool1 <- conv1
I1016 22:55:42.244854 32678 net.cpp:380] pool1 -> pool1
I1016 22:55:42.244890 32678 net.cpp:122] Setting up pool1
I1016 22:55:42.244895 32678 net.cpp:129] Top shape: 100 20 12 12 (288000)
I1016 22:55:42.244899 32678 net.cpp:137] Memory required for data: 6074800
I1016 22:55:42.244904 32678 layer_factory.hpp:77] Creating layer conv2
I1016 22:55:42.244910 32678 net.cpp:84] Creating Layer conv2
I1016 22:55:42.244915 32678 net.cpp:406] conv2 <- pool1
I1016 22:55:42.244918 32678 net.cpp:380] conv2 -> conv2
I1016 22:55:42.245888 32678 net.cpp:122] Setting up conv2
I1016 22:55:42.245896 32678 net.cpp:129] Top shape: 100 50 8 8 (320000)
I1016 22:55:42.245898 32678 net.cpp:137] Memory required for data: 7354800
I1016 22:55:42.245903 32678 layer_factory.hpp:77] Creating layer pool2
I1016 22:55:42.245909 32678 net.cpp:84] Creating Layer pool2
I1016 22:55:42.245913 32678 net.cpp:406] pool2 <- conv2
I1016 22:55:42.245915 32678 net.cpp:380] pool2 -> pool2
I1016 22:55:42.245937 32678 net.cpp:122] Setting up pool2
I1016 22:55:42.245942 32678 net.cpp:129] Top shape: 100 50 4 4 (80000)
I1016 22:55:42.245944 32678 net.cpp:137] Memory required for data: 7674800
I1016 22:55:42.245945 32678 layer_factory.hpp:77] Creating layer ip1
I1016 22:55:42.245950 32678 net.cpp:84] Creating Layer ip1
I1016 22:55:42.245952 32678 net.cpp:406] ip1 <- pool2
I1016 22:55:42.245956 32678 net.cpp:380] ip1 -> ip1
I1016 22:55:42.247757 32678 net.cpp:122] Setting up ip1
I1016 22:55:42.247766 32678 net.cpp:129] Top shape: 100 500 (50000)
I1016 22:55:42.247767 32678 net.cpp:137] Memory required for data: 7874800
I1016 22:55:42.247772 32678 layer_factory.hpp:77] Creating layer relu1
I1016 22:55:42.247787 32678 net.cpp:84] Creating Layer relu1
I1016 22:55:42.247789 32678 net.cpp:406] relu1 <- ip1
I1016 22:55:42.247793 32678 net.cpp:367] relu1 -> ip1 (in-place)
I1016 22:55:42.247910 32678 net.cpp:122] Setting up relu1
I1016 22:55:42.247915 32678 net.cpp:129] Top shape: 100 500 (50000)
I1016 22:55:42.247916 32678 net.cpp:137] Memory required for data: 8074800
I1016 22:55:42.247920 32678 layer_factory.hpp:77] Creating layer ip2
I1016 22:55:42.247925 32678 net.cpp:84] Creating Layer ip2
I1016 22:55:42.247927 32678 net.cpp:406] ip2 <- ip1
I1016 22:55:42.247931 32678 net.cpp:380] ip2 -> ip2
I1016 22:55:42.248013 32678 net.cpp:122] Setting up ip2
I1016 22:55:42.248019 32678 net.cpp:129] Top shape: 100 10 (1000)
I1016 22:55:42.248023 32678 net.cpp:137] Memory required for data: 8078800
I1016 22:55:42.248026 32678 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I1016 22:55:42.248030 32678 net.cpp:84] Creating Layer ip2_ip2_0_split
I1016 22:55:42.248034 32678 net.cpp:406] ip2_ip2_0_split <- ip2
I1016 22:55:42.248036 32678 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_0
I1016 22:55:42.248040 32678 net.cpp:380] ip2_ip2_0_split -> ip2_ip2_0_split_1
I1016 22:55:42.248081 32678 net.cpp:122] Setting up ip2_ip2_0_split
I1016 22:55:42.248085 32678 net.cpp:129] Top shape: 100 10 (1000)
I1016 22:55:42.248088 32678 net.cpp:129] Top shape: 100 10 (1000)
I1016 22:55:42.248100 32678 net.cpp:137] Memory required for data: 8086800
I1016 22:55:42.248102 32678 layer_factory.hpp:77] Creating layer accuracy
I1016 22:55:42.248107 32678 net.cpp:84] Creating Layer accuracy
I1016 22:55:42.248111 32678 net.cpp:406] accuracy <- ip2_ip2_0_split_0
I1016 22:55:42.248112 32678 net.cpp:406] accuracy <- label_mnist_1_split_0
I1016 22:55:42.248116 32678 net.cpp:380] accuracy -> accuracy
I1016 22:55:42.248122 32678 net.cpp:122] Setting up accuracy
I1016 22:55:42.248126 32678 net.cpp:129] Top shape: (1)
I1016 22:55:42.248128 32678 net.cpp:137] Memory required for data: 8086804
I1016 22:55:42.248131 32678 layer_factory.hpp:77] Creating layer loss
I1016 22:55:42.248133 32678 net.cpp:84] Creating Layer loss
I1016 22:55:42.248136 32678 net.cpp:406] loss <- ip2_ip2_0_split_1
I1016 22:55:42.248137 32678 net.cpp:406] loss <- label_mnist_1_split_1
I1016 22:55:42.248157 32678 net.cpp:380] loss -> loss
I1016 22:55:42.248173 32678 layer_factory.hpp:77] Creating layer loss
I1016 22:55:42.248543 32678 net.cpp:122] Setting up loss
I1016 22:55:42.248550 32678 net.cpp:129] Top shape: (1)
I1016 22:55:42.248553 32678 net.cpp:132]     with loss weight 1
I1016 22:55:42.248561 32678 net.cpp:137] Memory required for data: 8086808
I1016 22:55:42.248564 32678 net.cpp:198] loss needs backward computation.
I1016 22:55:42.248566 32678 net.cpp:200] accuracy does not need backward computation.
I1016 22:55:42.248569 32678 net.cpp:198] ip2_ip2_0_split needs backward computation.
I1016 22:55:42.248571 32678 net.cpp:198] ip2 needs backward computation.
I1016 22:55:42.248574 32678 net.cpp:198] relu1 needs backward computation.
I1016 22:55:42.248575 32678 net.cpp:198] ip1 needs backward computation.
I1016 22:55:42.248577 32678 net.cpp:198] pool2 needs backward computation.
I1016 22:55:42.248579 32678 net.cpp:198] conv2 needs backward computation.
I1016 22:55:42.248580 32678 net.cpp:198] pool1 needs backward computation.
I1016 22:55:42.248582 32678 net.cpp:198] conv1 needs backward computation.
I1016 22:55:42.248586 32678 net.cpp:200] label_mnist_1_split does not need backward computation.
I1016 22:55:42.248589 32678 net.cpp:200] mnist does not need backward computation.
I1016 22:55:42.248590 32678 net.cpp:242] This network produces output accuracy
I1016 22:55:42.248594 32678 net.cpp:242] This network produces output loss
I1016 22:55:42.248600 32678 net.cpp:255] Network initialization done.
I1016 22:55:42.248631 32678 solver.cpp:56] Solver scaffolding done.
I1016 22:55:42.248805 32678 caffe.cpp:248] Starting Optimization
I1016 22:55:42.248809 32678 solver.cpp:272] Solving LeNet
I1016 22:55:42.248811 32678 solver.cpp:273] Learning Rate Policy: inv
I1016 22:55:42.249322 32678 solver.cpp:330] Iteration 0, Testing net (#0)
I1016 22:55:42.252344 32678 blocking_queue.cpp:49] Waiting for data
I1016 22:55:42.372225 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:42.374483 32678 solver.cpp:397]     Test net output #0: accuracy = 0.1327
I1016 22:55:42.374505 32678 solver.cpp:397]     Test net output #1: loss = 2.43995 (* 1 = 2.43995 loss)
I1016 22:55:42.378370 32678 solver.cpp:218] Iteration 0 (-2.02052e+27 iter/s, 0.129536s/100 iters), loss = 2.50556
I1016 22:55:42.378388 32678 solver.cpp:237]     Train net output #0: loss = 2.50556 (* 1 = 2.50556 loss)
I1016 22:55:42.378402 32678 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1016 22:55:42.637167 32678 solver.cpp:218] Iteration 100 (386.446 iter/s, 0.258768s/100 iters), loss = 0.219427
I1016 22:55:42.637207 32678 solver.cpp:237]     Train net output #0: loss = 0.219427 (* 1 = 0.219427 loss)
I1016 22:55:42.637210 32678 sgd_solver.cpp:105] Iteration 100, lr = 0.00992565
I1016 22:55:42.882243 32678 solver.cpp:218] Iteration 200 (408.116 iter/s, 0.245028s/100 iters), loss = 0.163702
I1016 22:55:42.882270 32678 solver.cpp:237]     Train net output #0: loss = 0.163702 (* 1 = 0.163702 loss)
I1016 22:55:42.882275 32678 sgd_solver.cpp:105] Iteration 200, lr = 0.00985258
I1016 22:55:43.125680 32678 solver.cpp:218] Iteration 300 (410.846 iter/s, 0.2434s/100 iters), loss = 0.148581
I1016 22:55:43.125738 32678 solver.cpp:237]     Train net output #0: loss = 0.148581 (* 1 = 0.148581 loss)
I1016 22:55:43.125743 32678 sgd_solver.cpp:105] Iteration 300, lr = 0.00978075
I1016 22:55:43.367401 32678 solver.cpp:218] Iteration 400 (413.818 iter/s, 0.241652s/100 iters), loss = 0.0733697
I1016 22:55:43.367431 32678 solver.cpp:237]     Train net output #0: loss = 0.0733697 (* 1 = 0.0733697 loss)
I1016 22:55:43.367436 32678 sgd_solver.cpp:105] Iteration 400, lr = 0.00971013
I1016 22:55:43.607203 32678 solver.cpp:330] Iteration 500, Testing net (#0)
I1016 22:55:43.710505 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:43.713412 32678 solver.cpp:397]     Test net output #0: accuracy = 0.97
I1016 22:55:43.713433 32678 solver.cpp:397]     Test net output #1: loss = 0.0885912 (* 1 = 0.0885912 loss)
I1016 22:55:43.715725 32678 solver.cpp:218] Iteration 500 (287.118 iter/s, 0.348288s/100 iters), loss = 0.0889286
I1016 22:55:43.715744 32678 solver.cpp:237]     Train net output #0: loss = 0.0889287 (* 1 = 0.0889287 loss)
I1016 22:55:43.715754 32678 sgd_solver.cpp:105] Iteration 500, lr = 0.00964069
I1016 22:55:43.960825 32678 solver.cpp:218] Iteration 600 (408.049 iter/s, 0.245069s/100 iters), loss = 0.0861198
I1016 22:55:43.960855 32678 solver.cpp:237]     Train net output #0: loss = 0.0861198 (* 1 = 0.0861198 loss)
I1016 22:55:43.960870 32678 sgd_solver.cpp:105] Iteration 600, lr = 0.0095724
I1016 22:55:44.204375 32678 solver.cpp:218] Iteration 700 (410.66 iter/s, 0.243511s/100 iters), loss = 0.124727
I1016 22:55:44.204412 32678 solver.cpp:237]     Train net output #0: loss = 0.124727 (* 1 = 0.124727 loss)
I1016 22:55:44.204416 32678 sgd_solver.cpp:105] Iteration 700, lr = 0.00950522
I1016 22:55:44.446262 32678 solver.cpp:218] Iteration 800 (413.507 iter/s, 0.241834s/100 iters), loss = 0.189386
I1016 22:55:44.446300 32678 solver.cpp:237]     Train net output #0: loss = 0.189386 (* 1 = 0.189386 loss)
I1016 22:55:44.446305 32678 sgd_solver.cpp:105] Iteration 800, lr = 0.00943913
I1016 22:55:44.691428 32678 solver.cpp:218] Iteration 900 (407.967 iter/s, 0.245118s/100 iters), loss = 0.155243
I1016 22:55:44.691468 32678 solver.cpp:237]     Train net output #0: loss = 0.155243 (* 1 = 0.155243 loss)
I1016 22:55:44.691473 32678 sgd_solver.cpp:105] Iteration 900, lr = 0.00937411
I1016 22:55:44.774422 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:44.933336 32678 solver.cpp:330] Iteration 1000, Testing net (#0)
I1016 22:55:45.037833 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:45.041585 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9827
I1016 22:55:45.041604 32678 solver.cpp:397]     Test net output #1: loss = 0.0530257 (* 1 = 0.0530257 loss)
I1016 22:55:45.043828 32678 solver.cpp:218] Iteration 1000 (283.804 iter/s, 0.352356s/100 iters), loss = 0.0803936
I1016 22:55:45.043845 32678 solver.cpp:237]     Train net output #0: loss = 0.0803936 (* 1 = 0.0803936 loss)
I1016 22:55:45.043853 32678 sgd_solver.cpp:105] Iteration 1000, lr = 0.00931012
I1016 22:55:45.289284 32678 solver.cpp:218] Iteration 1100 (407.448 iter/s, 0.24543s/100 iters), loss = 0.00774366
I1016 22:55:45.289325 32678 solver.cpp:237]     Train net output #0: loss = 0.00774366 (* 1 = 0.00774366 loss)
I1016 22:55:45.289330 32678 sgd_solver.cpp:105] Iteration 1100, lr = 0.00924715
I1016 22:55:45.530865 32678 solver.cpp:218] Iteration 1200 (414.033 iter/s, 0.241527s/100 iters), loss = 0.0245872
I1016 22:55:45.530903 32678 solver.cpp:237]     Train net output #0: loss = 0.0245872 (* 1 = 0.0245872 loss)
I1016 22:55:45.530907 32678 sgd_solver.cpp:105] Iteration 1200, lr = 0.00918515
I1016 22:55:45.773460 32678 solver.cpp:218] Iteration 1300 (412.29 iter/s, 0.242547s/100 iters), loss = 0.0223724
I1016 22:55:45.773488 32678 solver.cpp:237]     Train net output #0: loss = 0.0223724 (* 1 = 0.0223724 loss)
I1016 22:55:45.773494 32678 sgd_solver.cpp:105] Iteration 1300, lr = 0.00912412
I1016 22:55:46.016024 32678 solver.cpp:218] Iteration 1400 (412.337 iter/s, 0.24252s/100 iters), loss = 0.00735695
I1016 22:55:46.016052 32678 solver.cpp:237]     Train net output #0: loss = 0.00735693 (* 1 = 0.00735693 loss)
I1016 22:55:46.016057 32678 sgd_solver.cpp:105] Iteration 1400, lr = 0.00906403
I1016 22:55:46.258565 32678 solver.cpp:330] Iteration 1500, Testing net (#0)
I1016 22:55:46.364558 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:46.367442 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9844
I1016 22:55:46.367472 32678 solver.cpp:397]     Test net output #1: loss = 0.0484766 (* 1 = 0.0484766 loss)
I1016 22:55:46.369736 32678 solver.cpp:218] Iteration 1500 (282.756 iter/s, 0.353661s/100 iters), loss = 0.0749661
I1016 22:55:46.369763 32678 solver.cpp:237]     Train net output #0: loss = 0.0749661 (* 1 = 0.0749661 loss)
I1016 22:55:46.369770 32678 sgd_solver.cpp:105] Iteration 1500, lr = 0.00900485
I1016 22:55:46.612861 32678 solver.cpp:218] Iteration 1600 (411.381 iter/s, 0.243084s/100 iters), loss = 0.14839
I1016 22:55:46.612892 32678 solver.cpp:237]     Train net output #0: loss = 0.14839 (* 1 = 0.14839 loss)
I1016 22:55:46.612908 32678 sgd_solver.cpp:105] Iteration 1600, lr = 0.00894657
I1016 22:55:46.857348 32678 solver.cpp:218] Iteration 1700 (409.088 iter/s, 0.244446s/100 iters), loss = 0.0373797
I1016 22:55:46.857386 32678 solver.cpp:237]     Train net output #0: loss = 0.0373797 (* 1 = 0.0373797 loss)
I1016 22:55:46.857391 32678 sgd_solver.cpp:105] Iteration 1700, lr = 0.00888916
I1016 22:55:47.098629 32678 solver.cpp:218] Iteration 1800 (414.544 iter/s, 0.241229s/100 iters), loss = 0.018548
I1016 22:55:47.098657 32678 solver.cpp:237]     Train net output #0: loss = 0.018548 (* 1 = 0.018548 loss)
I1016 22:55:47.098662 32678 sgd_solver.cpp:105] Iteration 1800, lr = 0.0088326
I1016 22:55:47.270370 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:47.344986 32678 solver.cpp:218] Iteration 1900 (405.977 iter/s, 0.246319s/100 iters), loss = 0.0985722
I1016 22:55:47.345026 32678 solver.cpp:237]     Train net output #0: loss = 0.0985722 (* 1 = 0.0985722 loss)
I1016 22:55:47.345031 32678 sgd_solver.cpp:105] Iteration 1900, lr = 0.00877687
I1016 22:55:47.583580 32678 solver.cpp:330] Iteration 2000, Testing net (#0)
I1016 22:55:47.688380 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:47.691268 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9865
I1016 22:55:47.691296 32678 solver.cpp:397]     Test net output #1: loss = 0.0416194 (* 1 = 0.0416194 loss)
I1016 22:55:47.693465 32678 solver.cpp:218] Iteration 2000 (286.998 iter/s, 0.348434s/100 iters), loss = 0.0108996
I1016 22:55:47.693490 32678 solver.cpp:237]     Train net output #0: loss = 0.0108997 (* 1 = 0.0108997 loss)
I1016 22:55:47.693496 32678 sgd_solver.cpp:105] Iteration 2000, lr = 0.00872196
I1016 22:55:47.936760 32678 solver.cpp:218] Iteration 2100 (411.08 iter/s, 0.243262s/100 iters), loss = 0.0133777
I1016 22:55:47.936799 32678 solver.cpp:237]     Train net output #0: loss = 0.0133777 (* 1 = 0.0133777 loss)
I1016 22:55:47.936802 32678 sgd_solver.cpp:105] Iteration 2100, lr = 0.00866784
I1016 22:55:48.179051 32678 solver.cpp:218] Iteration 2200 (412.829 iter/s, 0.242231s/100 iters), loss = 0.0111458
I1016 22:55:48.179090 32678 solver.cpp:237]     Train net output #0: loss = 0.0111459 (* 1 = 0.0111459 loss)
I1016 22:55:48.179095 32678 sgd_solver.cpp:105] Iteration 2200, lr = 0.0086145
I1016 22:55:48.421834 32678 solver.cpp:218] Iteration 2300 (411.969 iter/s, 0.242737s/100 iters), loss = 0.0887186
I1016 22:55:48.421874 32678 solver.cpp:237]     Train net output #0: loss = 0.0887186 (* 1 = 0.0887186 loss)
I1016 22:55:48.421878 32678 sgd_solver.cpp:105] Iteration 2300, lr = 0.00856192
I1016 22:55:48.663727 32678 solver.cpp:218] Iteration 2400 (413.495 iter/s, 0.241841s/100 iters), loss = 0.0114689
I1016 22:55:48.663753 32678 solver.cpp:237]     Train net output #0: loss = 0.0114689 (* 1 = 0.0114689 loss)
I1016 22:55:48.663758 32678 sgd_solver.cpp:105] Iteration 2400, lr = 0.00851008
I1016 22:55:48.903529 32678 solver.cpp:330] Iteration 2500, Testing net (#0)
I1016 22:55:49.006232 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:49.009068 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9863
I1016 22:55:49.009089 32678 solver.cpp:397]     Test net output #1: loss = 0.0431593 (* 1 = 0.0431593 loss)
I1016 22:55:49.011296 32678 solver.cpp:218] Iteration 2500 (287.739 iter/s, 0.347537s/100 iters), loss = 0.0211733
I1016 22:55:49.011322 32678 solver.cpp:237]     Train net output #0: loss = 0.0211733 (* 1 = 0.0211733 loss)
I1016 22:55:49.011329 32678 sgd_solver.cpp:105] Iteration 2500, lr = 0.00845897
I1016 22:55:49.254472 32678 solver.cpp:218] Iteration 2600 (411.293 iter/s, 0.243135s/100 iters), loss = 0.0572835
I1016 22:55:49.254509 32678 solver.cpp:237]     Train net output #0: loss = 0.0572835 (* 1 = 0.0572835 loss)
I1016 22:55:49.254535 32678 sgd_solver.cpp:105] Iteration 2600, lr = 0.00840857
I1016 22:55:49.496518 32678 solver.cpp:218] Iteration 2700 (413.224 iter/s, 0.241999s/100 iters), loss = 0.062085
I1016 22:55:49.496548 32678 solver.cpp:237]     Train net output #0: loss = 0.062085 (* 1 = 0.062085 loss)
I1016 22:55:49.496554 32678 sgd_solver.cpp:105] Iteration 2700, lr = 0.00835886
I1016 22:55:49.738350 32678 solver.cpp:218] Iteration 2800 (413.578 iter/s, 0.241792s/100 iters), loss = 0.00292006
I1016 22:55:49.738397 32678 solver.cpp:237]     Train net output #0: loss = 0.00292009 (* 1 = 0.00292009 loss)
I1016 22:55:49.738402 32678 sgd_solver.cpp:105] Iteration 2800, lr = 0.00830984
I1016 22:55:49.758462 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:49.981655 32678 solver.cpp:218] Iteration 2900 (411.104 iter/s, 0.243248s/100 iters), loss = 0.0219795
I1016 22:55:49.981686 32678 solver.cpp:237]     Train net output #0: loss = 0.0219795 (* 1 = 0.0219795 loss)
I1016 22:55:49.981693 32678 sgd_solver.cpp:105] Iteration 2900, lr = 0.00826148
I1016 22:55:50.220639 32678 solver.cpp:330] Iteration 3000, Testing net (#0)
I1016 22:55:50.324672 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:50.327591 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9873
I1016 22:55:50.327615 32678 solver.cpp:397]     Test net output #1: loss = 0.0354935 (* 1 = 0.0354935 loss)
I1016 22:55:50.329906 32678 solver.cpp:218] Iteration 3000 (287.177 iter/s, 0.348217s/100 iters), loss = 0.0107197
I1016 22:55:50.329944 32678 solver.cpp:237]     Train net output #0: loss = 0.0107197 (* 1 = 0.0107197 loss)
I1016 22:55:50.329964 32678 sgd_solver.cpp:105] Iteration 3000, lr = 0.00821377
I1016 22:55:50.571249 32678 solver.cpp:218] Iteration 3100 (414.413 iter/s, 0.241305s/100 iters), loss = 0.013511
I1016 22:55:50.571276 32678 solver.cpp:237]     Train net output #0: loss = 0.0135111 (* 1 = 0.0135111 loss)
I1016 22:55:50.571281 32678 sgd_solver.cpp:105] Iteration 3100, lr = 0.0081667
I1016 22:55:50.814393 32678 solver.cpp:218] Iteration 3200 (411.35 iter/s, 0.243102s/100 iters), loss = 0.00710868
I1016 22:55:50.814431 32678 solver.cpp:237]     Train net output #0: loss = 0.00710872 (* 1 = 0.00710872 loss)
I1016 22:55:50.814436 32678 sgd_solver.cpp:105] Iteration 3200, lr = 0.00812025
I1016 22:55:51.058845 32678 solver.cpp:218] Iteration 3300 (409.157 iter/s, 0.244405s/100 iters), loss = 0.0267991
I1016 22:55:51.058876 32678 solver.cpp:237]     Train net output #0: loss = 0.0267992 (* 1 = 0.0267992 loss)
I1016 22:55:51.058890 32678 sgd_solver.cpp:105] Iteration 3300, lr = 0.00807442
I1016 22:55:51.301344 32678 solver.cpp:218] Iteration 3400 (412.447 iter/s, 0.242456s/100 iters), loss = 0.00597515
I1016 22:55:51.301373 32678 solver.cpp:237]     Train net output #0: loss = 0.00597521 (* 1 = 0.00597521 loss)
I1016 22:55:51.301378 32678 sgd_solver.cpp:105] Iteration 3400, lr = 0.00802918
I1016 22:55:51.538676 32678 solver.cpp:330] Iteration 3500, Testing net (#0)
I1016 22:55:51.641571 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:51.644434 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9857
I1016 22:55:51.644464 32678 solver.cpp:397]     Test net output #1: loss = 0.0404423 (* 1 = 0.0404423 loss)
I1016 22:55:51.646663 32678 solver.cpp:218] Iteration 3500 (289.616 iter/s, 0.345285s/100 iters), loss = 0.00789597
I1016 22:55:51.646688 32678 solver.cpp:237]     Train net output #0: loss = 0.00789603 (* 1 = 0.00789603 loss)
I1016 22:55:51.646694 32678 sgd_solver.cpp:105] Iteration 3500, lr = 0.00798454
I1016 22:55:51.889853 32678 solver.cpp:218] Iteration 3600 (411.266 iter/s, 0.243152s/100 iters), loss = 0.030533
I1016 22:55:51.889879 32678 solver.cpp:237]     Train net output #0: loss = 0.030533 (* 1 = 0.030533 loss)
I1016 22:55:51.889884 32678 sgd_solver.cpp:105] Iteration 3600, lr = 0.00794046
I1016 22:55:52.134893 32678 solver.cpp:218] Iteration 3700 (408.158 iter/s, 0.245003s/100 iters), loss = 0.0269352
I1016 22:55:52.134920 32678 solver.cpp:237]     Train net output #0: loss = 0.0269352 (* 1 = 0.0269352 loss)
I1016 22:55:52.134953 32678 sgd_solver.cpp:105] Iteration 3700, lr = 0.00789695
I1016 22:55:52.247355 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:52.383136 32678 solver.cpp:218] Iteration 3800 (402.897 iter/s, 0.248202s/100 iters), loss = 0.00878664
I1016 22:55:52.383164 32678 solver.cpp:237]     Train net output #0: loss = 0.00878669 (* 1 = 0.00878669 loss)
I1016 22:55:52.383168 32678 sgd_solver.cpp:105] Iteration 3800, lr = 0.007854
I1016 22:55:52.630609 32678 solver.cpp:218] Iteration 3900 (404.146 iter/s, 0.247435s/100 iters), loss = 0.0383549
I1016 22:55:52.630636 32678 solver.cpp:237]     Train net output #0: loss = 0.038355 (* 1 = 0.038355 loss)
I1016 22:55:52.630640 32678 sgd_solver.cpp:105] Iteration 3900, lr = 0.00781158
I1016 22:55:52.871834 32678 solver.cpp:330] Iteration 4000, Testing net (#0)
I1016 22:55:52.975420 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:52.977272 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9895
I1016 22:55:52.977291 32678 solver.cpp:397]     Test net output #1: loss = 0.03009 (* 1 = 0.03009 loss)
I1016 22:55:52.979528 32678 solver.cpp:218] Iteration 4000 (286.624 iter/s, 0.348889s/100 iters), loss = 0.0118839
I1016 22:55:52.979554 32678 solver.cpp:237]     Train net output #0: loss = 0.0118839 (* 1 = 0.0118839 loss)
I1016 22:55:52.979562 32678 sgd_solver.cpp:105] Iteration 4000, lr = 0.0077697
I1016 22:55:53.222764 32678 solver.cpp:218] Iteration 4100 (411.18 iter/s, 0.243202s/100 iters), loss = 0.0473739
I1016 22:55:53.222800 32678 solver.cpp:237]     Train net output #0: loss = 0.0473739 (* 1 = 0.0473739 loss)
I1016 22:55:53.222805 32678 sgd_solver.cpp:105] Iteration 4100, lr = 0.00772833
I1016 22:55:53.464534 32678 solver.cpp:218] Iteration 4200 (413.702 iter/s, 0.24172s/100 iters), loss = 0.00998378
I1016 22:55:53.464572 32678 solver.cpp:237]     Train net output #0: loss = 0.00998378 (* 1 = 0.00998378 loss)
I1016 22:55:53.464576 32678 sgd_solver.cpp:105] Iteration 4200, lr = 0.00768748
I1016 22:55:53.706409 32678 solver.cpp:218] Iteration 4300 (413.518 iter/s, 0.241827s/100 iters), loss = 0.0567639
I1016 22:55:53.706447 32678 solver.cpp:237]     Train net output #0: loss = 0.0567639 (* 1 = 0.0567639 loss)
I1016 22:55:53.706452 32678 sgd_solver.cpp:105] Iteration 4300, lr = 0.00764712
I1016 22:55:53.948909 32678 solver.cpp:218] Iteration 4400 (412.464 iter/s, 0.242445s/100 iters), loss = 0.012092
I1016 22:55:53.948940 32678 solver.cpp:237]     Train net output #0: loss = 0.012092 (* 1 = 0.012092 loss)
I1016 22:55:53.948945 32678 sgd_solver.cpp:105] Iteration 4400, lr = 0.00760726
I1016 22:55:54.189146 32678 solver.cpp:330] Iteration 4500, Testing net (#0)
I1016 22:55:54.292012 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:54.294879 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9881
I1016 22:55:54.294909 32678 solver.cpp:397]     Test net output #1: loss = 0.0354301 (* 1 = 0.0354301 loss)
I1016 22:55:54.297139 32678 solver.cpp:218] Iteration 4500 (287.195 iter/s, 0.348196s/100 iters), loss = 0.00754549
I1016 22:55:54.297157 32678 solver.cpp:237]     Train net output #0: loss = 0.00754548 (* 1 = 0.00754548 loss)
I1016 22:55:54.297163 32678 sgd_solver.cpp:105] Iteration 4500, lr = 0.00756788
I1016 22:55:54.540457 32678 solver.cpp:218] Iteration 4600 (411.04 iter/s, 0.243285s/100 iters), loss = 0.0148845
I1016 22:55:54.540495 32678 solver.cpp:237]     Train net output #0: loss = 0.0148844 (* 1 = 0.0148844 loss)
I1016 22:55:54.540499 32678 sgd_solver.cpp:105] Iteration 4600, lr = 0.00752897
I1016 22:55:54.743943 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:54.786479 32678 solver.cpp:218] Iteration 4700 (406.546 iter/s, 0.245975s/100 iters), loss = 0.00882014
I1016 22:55:54.786520 32678 solver.cpp:237]     Train net output #0: loss = 0.00882013 (* 1 = 0.00882013 loss)
I1016 22:55:54.786523 32678 sgd_solver.cpp:105] Iteration 4700, lr = 0.00749052
I1016 22:55:55.028678 32678 solver.cpp:218] Iteration 4800 (412.973 iter/s, 0.242146s/100 iters), loss = 0.0138642
I1016 22:55:55.028733 32678 solver.cpp:237]     Train net output #0: loss = 0.0138642 (* 1 = 0.0138642 loss)
I1016 22:55:55.028739 32678 sgd_solver.cpp:105] Iteration 4800, lr = 0.00745253
I1016 22:55:55.271986 32678 solver.cpp:218] Iteration 4900 (411.108 iter/s, 0.243245s/100 iters), loss = 0.00711716
I1016 22:55:55.272024 32678 solver.cpp:237]     Train net output #0: loss = 0.00711716 (* 1 = 0.00711716 loss)
I1016 22:55:55.272029 32678 sgd_solver.cpp:105] Iteration 4900, lr = 0.00741498
I1016 22:55:55.509665 32678 solver.cpp:447] Snapshotting to binary proto file ../examples/mnist/build/snapshot/lenet_iter_5000.caffemodel
I1016 22:55:55.516551 32678 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../examples/mnist/build/snapshot/lenet_iter_5000.solverstate
I1016 22:55:55.518841 32678 solver.cpp:330] Iteration 5000, Testing net (#0)
I1016 22:55:55.620219 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:55.623039 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9895
I1016 22:55:55.623067 32678 solver.cpp:397]     Test net output #1: loss = 0.0289836 (* 1 = 0.0289836 loss)
I1016 22:55:55.625260 32678 solver.cpp:218] Iteration 5000 (283.101 iter/s, 0.35323s/100 iters), loss = 0.0204685
I1016 22:55:55.625288 32678 solver.cpp:237]     Train net output #0: loss = 0.0204685 (* 1 = 0.0204685 loss)
I1016 22:55:55.625293 32678 sgd_solver.cpp:105] Iteration 5000, lr = 0.00737788
I1016 22:55:55.867185 32678 solver.cpp:218] Iteration 5100 (413.395 iter/s, 0.241899s/100 iters), loss = 0.0154366
I1016 22:55:55.867213 32678 solver.cpp:237]     Train net output #0: loss = 0.0154366 (* 1 = 0.0154366 loss)
I1016 22:55:55.867218 32678 sgd_solver.cpp:105] Iteration 5100, lr = 0.0073412
I1016 22:55:56.108975 32678 solver.cpp:218] Iteration 5200 (413.645 iter/s, 0.241753s/100 iters), loss = 0.00807089
I1016 22:55:56.109012 32678 solver.cpp:237]     Train net output #0: loss = 0.00807089 (* 1 = 0.00807089 loss)
I1016 22:55:56.109017 32678 sgd_solver.cpp:105] Iteration 5200, lr = 0.00730495
I1016 22:55:56.350643 32678 solver.cpp:218] Iteration 5300 (413.893 iter/s, 0.241608s/100 iters), loss = 0.000983865
I1016 22:55:56.350682 32678 solver.cpp:237]     Train net output #0: loss = 0.000983864 (* 1 = 0.000983864 loss)
I1016 22:55:56.350698 32678 sgd_solver.cpp:105] Iteration 5300, lr = 0.00726911
I1016 22:55:56.593904 32678 solver.cpp:218] Iteration 5400 (411.164 iter/s, 0.243212s/100 iters), loss = 0.00746403
I1016 22:55:56.593941 32678 solver.cpp:237]     Train net output #0: loss = 0.00746403 (* 1 = 0.00746403 loss)
I1016 22:55:56.593945 32678 sgd_solver.cpp:105] Iteration 5400, lr = 0.00723368
I1016 22:55:56.833952 32678 solver.cpp:330] Iteration 5500, Testing net (#0)
I1016 22:55:56.938351 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:56.941193 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9899
I1016 22:55:56.941215 32678 solver.cpp:397]     Test net output #1: loss = 0.0317338 (* 1 = 0.0317338 loss)
I1016 22:55:56.943440 32678 solver.cpp:218] Iteration 5500 (286.127 iter/s, 0.349495s/100 iters), loss = 0.00647381
I1016 22:55:56.943465 32678 solver.cpp:237]     Train net output #0: loss = 0.00647381 (* 1 = 0.00647381 loss)
I1016 22:55:56.943471 32678 sgd_solver.cpp:105] Iteration 5500, lr = 0.00719865
I1016 22:55:57.185544 32678 solver.cpp:218] Iteration 5600 (413.108 iter/s, 0.242068s/100 iters), loss = 0.00135098
I1016 22:55:57.185572 32678 solver.cpp:237]     Train net output #0: loss = 0.00135098 (* 1 = 0.00135098 loss)
I1016 22:55:57.185577 32678 sgd_solver.cpp:105] Iteration 5600, lr = 0.00716402
I1016 22:55:57.237030 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:57.430938 32678 solver.cpp:218] Iteration 5700 (407.571 iter/s, 0.245356s/100 iters), loss = 0.00506267
I1016 22:55:57.430976 32678 solver.cpp:237]     Train net output #0: loss = 0.00506267 (* 1 = 0.00506267 loss)
I1016 22:55:57.430980 32678 sgd_solver.cpp:105] Iteration 5700, lr = 0.00712977
I1016 22:55:57.672417 32678 solver.cpp:218] Iteration 5800 (414.202 iter/s, 0.241428s/100 iters), loss = 0.0243678
I1016 22:55:57.672453 32678 solver.cpp:237]     Train net output #0: loss = 0.0243678 (* 1 = 0.0243678 loss)
I1016 22:55:57.672458 32678 sgd_solver.cpp:105] Iteration 5800, lr = 0.0070959
I1016 22:55:57.915369 32678 solver.cpp:218] Iteration 5900 (411.682 iter/s, 0.242906s/100 iters), loss = 0.0064101
I1016 22:55:57.915401 32678 solver.cpp:237]     Train net output #0: loss = 0.0064101 (* 1 = 0.0064101 loss)
I1016 22:55:57.915407 32678 sgd_solver.cpp:105] Iteration 5900, lr = 0.0070624
I1016 22:55:58.153372 32678 solver.cpp:330] Iteration 6000, Testing net (#0)
I1016 22:55:58.255954 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:58.258422 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9909
I1016 22:55:58.258442 32678 solver.cpp:397]     Test net output #1: loss = 0.0278629 (* 1 = 0.0278629 loss)
I1016 22:55:58.260654 32678 solver.cpp:218] Iteration 6000 (289.645 iter/s, 0.34525s/100 iters), loss = 0.00400403
I1016 22:55:58.260673 32678 solver.cpp:237]     Train net output #0: loss = 0.00400403 (* 1 = 0.00400403 loss)
I1016 22:55:58.260684 32678 sgd_solver.cpp:105] Iteration 6000, lr = 0.00702927
I1016 22:55:58.502053 32678 solver.cpp:218] Iteration 6100 (414.305 iter/s, 0.241368s/100 iters), loss = 0.00100505
I1016 22:55:58.502090 32678 solver.cpp:237]     Train net output #0: loss = 0.00100504 (* 1 = 0.00100504 loss)
I1016 22:55:58.502095 32678 sgd_solver.cpp:105] Iteration 6100, lr = 0.0069965
I1016 22:55:58.743998 32678 solver.cpp:218] Iteration 6200 (413.405 iter/s, 0.241894s/100 iters), loss = 0.00868662
I1016 22:55:58.744024 32678 solver.cpp:237]     Train net output #0: loss = 0.00868661 (* 1 = 0.00868661 loss)
I1016 22:55:58.744029 32678 sgd_solver.cpp:105] Iteration 6200, lr = 0.00696408
I1016 22:55:58.985729 32678 solver.cpp:218] Iteration 6300 (413.781 iter/s, 0.241674s/100 iters), loss = 0.0100463
I1016 22:55:58.985767 32678 solver.cpp:237]     Train net output #0: loss = 0.0100463 (* 1 = 0.0100463 loss)
I1016 22:55:58.985782 32678 sgd_solver.cpp:105] Iteration 6300, lr = 0.00693201
I1016 22:55:59.230201 32678 solver.cpp:218] Iteration 6400 (409.135 iter/s, 0.244418s/100 iters), loss = 0.00303193
I1016 22:55:59.230228 32678 solver.cpp:237]     Train net output #0: loss = 0.00303192 (* 1 = 0.00303192 loss)
I1016 22:55:59.230232 32678 sgd_solver.cpp:105] Iteration 6400, lr = 0.00690029
I1016 22:55:59.468606 32678 solver.cpp:330] Iteration 6500, Testing net (#0)
I1016 22:55:59.573215 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:59.576871 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9897
I1016 22:55:59.576900 32678 solver.cpp:397]     Test net output #1: loss = 0.0319682 (* 1 = 0.0319682 loss)
I1016 22:55:59.579123 32678 solver.cpp:218] Iteration 6500 (286.624 iter/s, 0.348889s/100 iters), loss = 0.00518374
I1016 22:55:59.579149 32678 solver.cpp:237]     Train net output #0: loss = 0.00518371 (* 1 = 0.00518371 loss)
I1016 22:55:59.579154 32678 sgd_solver.cpp:105] Iteration 6500, lr = 0.0068689
I1016 22:55:59.722201 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:55:59.825373 32678 solver.cpp:218] Iteration 6600 (406.152 iter/s, 0.246213s/100 iters), loss = 0.0152056
I1016 22:55:59.825402 32678 solver.cpp:237]     Train net output #0: loss = 0.0152056 (* 1 = 0.0152056 loss)
I1016 22:55:59.825408 32678 sgd_solver.cpp:105] Iteration 6600, lr = 0.00683784
I1016 22:56:00.066241 32678 solver.cpp:218] Iteration 6700 (415.23 iter/s, 0.24083s/100 iters), loss = 0.00831207
I1016 22:56:00.066269 32678 solver.cpp:237]     Train net output #0: loss = 0.00831205 (* 1 = 0.00831205 loss)
I1016 22:56:00.066285 32678 sgd_solver.cpp:105] Iteration 6700, lr = 0.00680711
I1016 22:56:00.310106 32678 solver.cpp:218] Iteration 6800 (410.136 iter/s, 0.243821s/100 iters), loss = 0.00405876
I1016 22:56:00.310135 32678 solver.cpp:237]     Train net output #0: loss = 0.00405873 (* 1 = 0.00405873 loss)
I1016 22:56:00.310168 32678 sgd_solver.cpp:105] Iteration 6800, lr = 0.0067767
I1016 22:56:00.550957 32678 solver.cpp:218] Iteration 6900 (415.264 iter/s, 0.240811s/100 iters), loss = 0.00382008
I1016 22:56:00.550997 32678 solver.cpp:237]     Train net output #0: loss = 0.00382005 (* 1 = 0.00382005 loss)
I1016 22:56:00.551002 32678 sgd_solver.cpp:105] Iteration 6900, lr = 0.0067466
I1016 22:56:00.788774 32678 solver.cpp:330] Iteration 7000, Testing net (#0)
I1016 22:56:00.893802 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:00.896627 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9898
I1016 22:56:00.896647 32678 solver.cpp:397]     Test net output #1: loss = 0.0296052 (* 1 = 0.0296052 loss)
I1016 22:56:00.898843 32678 solver.cpp:218] Iteration 7000 (287.489 iter/s, 0.34784s/100 iters), loss = 0.00681233
I1016 22:56:00.898869 32678 solver.cpp:237]     Train net output #0: loss = 0.0068123 (* 1 = 0.0068123 loss)
I1016 22:56:00.898875 32678 sgd_solver.cpp:105] Iteration 7000, lr = 0.00671681
I1016 22:56:01.141124 32678 solver.cpp:218] Iteration 7100 (412.81 iter/s, 0.242242s/100 iters), loss = 0.0150126
I1016 22:56:01.141163 32678 solver.cpp:237]     Train net output #0: loss = 0.0150126 (* 1 = 0.0150126 loss)
I1016 22:56:01.141167 32678 sgd_solver.cpp:105] Iteration 7100, lr = 0.00668733
I1016 22:56:01.383864 32678 solver.cpp:218] Iteration 7200 (412.051 iter/s, 0.242688s/100 iters), loss = 0.00522184
I1016 22:56:01.383893 32678 solver.cpp:237]     Train net output #0: loss = 0.0052218 (* 1 = 0.0052218 loss)
I1016 22:56:01.383898 32678 sgd_solver.cpp:105] Iteration 7200, lr = 0.00665815
I1016 22:56:01.626803 32678 solver.cpp:218] Iteration 7300 (411.692 iter/s, 0.2429s/100 iters), loss = 0.0118395
I1016 22:56:01.626842 32678 solver.cpp:237]     Train net output #0: loss = 0.0118395 (* 1 = 0.0118395 loss)
I1016 22:56:01.626847 32678 sgd_solver.cpp:105] Iteration 7300, lr = 0.00662927
I1016 22:56:01.870591 32678 solver.cpp:218] Iteration 7400 (410.281 iter/s, 0.243736s/100 iters), loss = 0.00674359
I1016 22:56:01.870625 32678 solver.cpp:237]     Train net output #0: loss = 0.00674356 (* 1 = 0.00674356 loss)
I1016 22:56:01.870631 32678 sgd_solver.cpp:105] Iteration 7400, lr = 0.00660067
I1016 22:56:02.101569 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:02.109544 32678 solver.cpp:330] Iteration 7500, Testing net (#0)
I1016 22:56:02.213685 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:02.215543 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9902
I1016 22:56:02.215570 32678 solver.cpp:397]     Test net output #1: loss = 0.0321909 (* 1 = 0.0321909 loss)
I1016 22:56:02.217834 32678 solver.cpp:218] Iteration 7500 (288.014 iter/s, 0.347206s/100 iters), loss = 0.0033038
I1016 22:56:02.217850 32678 solver.cpp:237]     Train net output #0: loss = 0.00330378 (* 1 = 0.00330378 loss)
I1016 22:56:02.217856 32678 sgd_solver.cpp:105] Iteration 7500, lr = 0.00657236
I1016 22:56:02.461778 32678 solver.cpp:218] Iteration 7600 (409.985 iter/s, 0.243912s/100 iters), loss = 0.00469021
I1016 22:56:02.461819 32678 solver.cpp:237]     Train net output #0: loss = 0.00469018 (* 1 = 0.00469018 loss)
I1016 22:56:02.461824 32678 sgd_solver.cpp:105] Iteration 7600, lr = 0.00654433
I1016 22:56:02.703544 32678 solver.cpp:218] Iteration 7700 (413.7 iter/s, 0.241721s/100 iters), loss = 0.0244868
I1016 22:56:02.703572 32678 solver.cpp:237]     Train net output #0: loss = 0.0244868 (* 1 = 0.0244868 loss)
I1016 22:56:02.703577 32678 sgd_solver.cpp:105] Iteration 7700, lr = 0.00651658
I1016 22:56:02.944408 32678 solver.cpp:218] Iteration 7800 (415.245 iter/s, 0.240821s/100 iters), loss = 0.00535327
I1016 22:56:02.944447 32678 solver.cpp:237]     Train net output #0: loss = 0.00535324 (* 1 = 0.00535324 loss)
I1016 22:56:02.944452 32678 sgd_solver.cpp:105] Iteration 7800, lr = 0.00648911
I1016 22:56:03.185937 32678 solver.cpp:218] Iteration 7900 (414.112 iter/s, 0.241481s/100 iters), loss = 0.0055459
I1016 22:56:03.185977 32678 solver.cpp:237]     Train net output #0: loss = 0.00554587 (* 1 = 0.00554587 loss)
I1016 22:56:03.186017 32678 sgd_solver.cpp:105] Iteration 7900, lr = 0.0064619
I1016 22:56:03.425683 32678 solver.cpp:330] Iteration 8000, Testing net (#0)
I1016 22:56:03.530551 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:03.534298 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9906
I1016 22:56:03.534329 32678 solver.cpp:397]     Test net output #1: loss = 0.028852 (* 1 = 0.028852 loss)
I1016 22:56:03.536486 32678 solver.cpp:218] Iteration 8000 (285.304 iter/s, 0.350504s/100 iters), loss = 0.00784842
I1016 22:56:03.536521 32678 solver.cpp:237]     Train net output #0: loss = 0.0078484 (* 1 = 0.0078484 loss)
I1016 22:56:03.536527 32678 sgd_solver.cpp:105] Iteration 8000, lr = 0.00643496
I1016 22:56:03.779920 32678 solver.cpp:218] Iteration 8100 (410.858 iter/s, 0.243393s/100 iters), loss = 0.0106244
I1016 22:56:03.779952 32678 solver.cpp:237]     Train net output #0: loss = 0.0106244 (* 1 = 0.0106244 loss)
I1016 22:56:03.779958 32678 sgd_solver.cpp:105] Iteration 8100, lr = 0.00640827
I1016 22:56:04.023180 32678 solver.cpp:218] Iteration 8200 (411.159 iter/s, 0.243215s/100 iters), loss = 0.0056975
I1016 22:56:04.023218 32678 solver.cpp:237]     Train net output #0: loss = 0.00569747 (* 1 = 0.00569747 loss)
I1016 22:56:04.023222 32678 sgd_solver.cpp:105] Iteration 8200, lr = 0.00638185
I1016 22:56:04.266176 32678 solver.cpp:218] Iteration 8300 (411.611 iter/s, 0.242948s/100 iters), loss = 0.0193155
I1016 22:56:04.266204 32678 solver.cpp:237]     Train net output #0: loss = 0.0193155 (* 1 = 0.0193155 loss)
I1016 22:56:04.266208 32678 sgd_solver.cpp:105] Iteration 8300, lr = 0.00635567
I1016 22:56:04.507596 32678 solver.cpp:218] Iteration 8400 (414.288 iter/s, 0.241378s/100 iters), loss = 0.00620089
I1016 22:56:04.507634 32678 solver.cpp:237]     Train net output #0: loss = 0.00620086 (* 1 = 0.00620086 loss)
I1016 22:56:04.507638 32678 sgd_solver.cpp:105] Iteration 8400, lr = 0.00632975
I1016 22:56:04.589463 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:04.748301 32678 solver.cpp:330] Iteration 8500, Testing net (#0)
I1016 22:56:04.851812 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:04.854706 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9907
I1016 22:56:04.854728 32678 solver.cpp:397]     Test net output #1: loss = 0.028599 (* 1 = 0.028599 loss)
I1016 22:56:04.856947 32678 solver.cpp:218] Iteration 8500 (286.28 iter/s, 0.349308s/100 iters), loss = 0.0071484
I1016 22:56:04.856966 32678 solver.cpp:237]     Train net output #0: loss = 0.00714836 (* 1 = 0.00714836 loss)
I1016 22:56:04.856973 32678 sgd_solver.cpp:105] Iteration 8500, lr = 0.00630407
I1016 22:56:05.099942 32678 solver.cpp:218] Iteration 8600 (411.586 iter/s, 0.242963s/100 iters), loss = 0.000760476
I1016 22:56:05.099980 32678 solver.cpp:237]     Train net output #0: loss = 0.000760427 (* 1 = 0.000760427 loss)
I1016 22:56:05.099984 32678 sgd_solver.cpp:105] Iteration 8600, lr = 0.00627864
I1016 22:56:05.342155 32678 solver.cpp:218] Iteration 8700 (412.943 iter/s, 0.242164s/100 iters), loss = 0.00279577
I1016 22:56:05.342193 32678 solver.cpp:237]     Train net output #0: loss = 0.00279572 (* 1 = 0.00279572 loss)
I1016 22:56:05.342197 32678 sgd_solver.cpp:105] Iteration 8700, lr = 0.00625344
I1016 22:56:05.584337 32678 solver.cpp:218] Iteration 8800 (413 iter/s, 0.242131s/100 iters), loss = 0.0013403
I1016 22:56:05.584365 32678 solver.cpp:237]     Train net output #0: loss = 0.00134025 (* 1 = 0.00134025 loss)
I1016 22:56:05.584370 32678 sgd_solver.cpp:105] Iteration 8800, lr = 0.00622847
I1016 22:56:05.827126 32678 solver.cpp:218] Iteration 8900 (411.945 iter/s, 0.242751s/100 iters), loss = 0.00046643
I1016 22:56:05.827157 32678 solver.cpp:237]     Train net output #0: loss = 0.000466379 (* 1 = 0.000466379 loss)
I1016 22:56:05.827163 32678 sgd_solver.cpp:105] Iteration 8900, lr = 0.00620374
I1016 22:56:06.068260 32678 solver.cpp:330] Iteration 9000, Testing net (#0)
I1016 22:56:06.173892 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:06.176769 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9913
I1016 22:56:06.176791 32678 solver.cpp:397]     Test net output #1: loss = 0.0281317 (* 1 = 0.0281317 loss)
I1016 22:56:06.178966 32678 solver.cpp:218] Iteration 9000 (284.251 iter/s, 0.351802s/100 iters), loss = 0.0156007
I1016 22:56:06.178983 32678 solver.cpp:237]     Train net output #0: loss = 0.0156006 (* 1 = 0.0156006 loss)
I1016 22:56:06.179003 32678 sgd_solver.cpp:105] Iteration 9000, lr = 0.00617924
I1016 22:56:06.421742 32678 solver.cpp:218] Iteration 9100 (411.977 iter/s, 0.242732s/100 iters), loss = 0.00918844
I1016 22:56:06.421789 32678 solver.cpp:237]     Train net output #0: loss = 0.00918839 (* 1 = 0.00918839 loss)
I1016 22:56:06.421794 32678 sgd_solver.cpp:105] Iteration 9100, lr = 0.00615496
I1016 22:56:06.665339 32678 solver.cpp:218] Iteration 9200 (410.618 iter/s, 0.243535s/100 iters), loss = 0.00272752
I1016 22:56:06.665380 32678 solver.cpp:237]     Train net output #0: loss = 0.00272746 (* 1 = 0.00272746 loss)
I1016 22:56:06.665383 32678 sgd_solver.cpp:105] Iteration 9200, lr = 0.0061309
I1016 22:56:06.905360 32678 solver.cpp:218] Iteration 9300 (416.709 iter/s, 0.239975s/100 iters), loss = 0.00639689
I1016 22:56:06.905390 32678 solver.cpp:237]     Train net output #0: loss = 0.00639683 (* 1 = 0.00639683 loss)
I1016 22:56:06.905395 32678 sgd_solver.cpp:105] Iteration 9300, lr = 0.00610706
I1016 22:56:07.076494 32685 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:07.152848 32678 solver.cpp:218] Iteration 9400 (404.195 iter/s, 0.247405s/100 iters), loss = 0.016794
I1016 22:56:07.152956 32678 solver.cpp:237]     Train net output #0: loss = 0.016794 (* 1 = 0.016794 loss)
I1016 22:56:07.152982 32678 sgd_solver.cpp:105] Iteration 9400, lr = 0.00608343
I1016 22:56:07.397349 32678 solver.cpp:330] Iteration 9500, Testing net (#0)
I1016 22:56:07.502951 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:07.505833 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9895
I1016 22:56:07.505852 32678 solver.cpp:397]     Test net output #1: loss = 0.0337481 (* 1 = 0.0337481 loss)
I1016 22:56:07.508158 32678 solver.cpp:218] Iteration 9500 (281.529 iter/s, 0.355204s/100 iters), loss = 0.00178882
I1016 22:56:07.508175 32678 solver.cpp:237]     Train net output #0: loss = 0.00178876 (* 1 = 0.00178876 loss)
I1016 22:56:07.508183 32678 sgd_solver.cpp:105] Iteration 9500, lr = 0.00606002
I1016 22:56:07.753293 32678 solver.cpp:218] Iteration 9600 (407.99 iter/s, 0.245104s/100 iters), loss = 0.00145912
I1016 22:56:07.753324 32678 solver.cpp:237]     Train net output #0: loss = 0.00145905 (* 1 = 0.00145905 loss)
I1016 22:56:07.753340 32678 sgd_solver.cpp:105] Iteration 9600, lr = 0.00603682
I1016 22:56:08.000447 32678 solver.cpp:218] Iteration 9700 (404.668 iter/s, 0.247116s/100 iters), loss = 0.00301991
I1016 22:56:08.000478 32678 solver.cpp:237]     Train net output #0: loss = 0.00301985 (* 1 = 0.00301985 loss)
I1016 22:56:08.000494 32678 sgd_solver.cpp:105] Iteration 9700, lr = 0.00601382
I1016 22:56:08.243758 32678 solver.cpp:218] Iteration 9800 (411.063 iter/s, 0.243272s/100 iters), loss = 0.010448
I1016 22:56:08.243798 32678 solver.cpp:237]     Train net output #0: loss = 0.010448 (* 1 = 0.010448 loss)
I1016 22:56:08.243803 32678 sgd_solver.cpp:105] Iteration 9800, lr = 0.00599102
I1016 22:56:08.488339 32678 solver.cpp:218] Iteration 9900 (408.947 iter/s, 0.244531s/100 iters), loss = 0.00341326
I1016 22:56:08.488378 32678 solver.cpp:237]     Train net output #0: loss = 0.0034132 (* 1 = 0.0034132 loss)
I1016 22:56:08.488381 32678 sgd_solver.cpp:105] Iteration 9900, lr = 0.00596843
I1016 22:56:08.728754 32678 solver.cpp:447] Snapshotting to binary proto file ../examples/mnist/build/snapshot/lenet_iter_10000.caffemodel
I1016 22:56:08.734764 32678 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ../examples/mnist/build/snapshot/lenet_iter_10000.solverstate
I1016 22:56:08.737743 32678 solver.cpp:310] Iteration 10000, loss = 0.00316732
I1016 22:56:08.737761 32678 solver.cpp:330] Iteration 10000, Testing net (#0)
I1016 22:56:08.841251 32686 data_layer.cpp:73] Restarting data prefetching from start.
I1016 22:56:08.844082 32678 solver.cpp:397]     Test net output #0: accuracy = 0.9909
I1016 22:56:08.844113 32678 solver.cpp:397]     Test net output #1: loss = 0.02833 (* 1 = 0.02833 loss)
I1016 22:56:08.844120 32678 solver.cpp:315] Optimization Done.
I1016 22:56:08.844122 32678 caffe.cpp:259] Optimization Done.
